XGBoost Hyperparameter Tuning - 


Deal with overfitting
---------------------
Use small max_bin
Use small num_leaves
Use min_data_in_leaf and min_sum_hessian_in_leaf
Use bagging by set bagging_fraction and bagging_freq
Use feature sub-sampling by set feature_fraction
Use bigger training data
Try lambda_l1, lambda_l2 and min_gain_to_split for regularization
Try max_depth to avoid growing deep tree

Higher Accuracy
---------------
Use large max_bin 
Use small learning_rate with large num_iterations
Use large num_leaves (may cause over-fitting)
Use bigger training data
Try dart


num_leaves = 2^(max_depth)
16384 = 2 power(14)


Understand the parameters 
-------------------------
'num_leaves': 10, - large for overfitting
'max_bin': 119, - large for overfitting :) (may be slower)
'min_data_in_leaf': 11,  - large for overfitting. keep it 100 to 1000 for large datasets
'learning_rate': 0.02,  - keep it minimum for slow training but accurate
'min_sum_hessian_in_leaf': 0.00245,

# Regularization - makes the output regular instead of over fluctuating. 
'lambda_l1': 4.972,
'lambda_l2': 2.276,
'min_gain_to_split': 0.65,

#This can be increased for overfitting
'max_depth': 14,

var_63


Step 1 : Use the below parameters
param = {
        silent=False, 
        scale_pos_weight=1,
        learning_rate=0.01,  
        colsample_bytree = 0.4,
        subsample = 0.8,
        objective='binary:logistic', 
        n_estimators=1000,  #100 if the size of your data is high, 1000 is if it is medium-low
        reg_alpha = 0.3,
        max_depth=4, 
        gamma=10
}

Run model.fit(eval_set, eval_metric) and diagnose your first run, specifically the n_estimators parameter

Step 2 : Use the below parameters
Optimize max_depth parameter. It represents the depth of each tree, which is the maximum number of different features used in each tree. I recommend going from a low max_depth (3 for instance) and then increasing it incrementally by 1, and stopping when there’s no performance gain of increasing it. This will help simplify your model and avoid overfitting

Step 3 : 
Now play around with the learning rate and the features that avoids overfitting:

learning_rate: usually between 0.1 and 0.01. If you’re focused on performance and have time in front of you, decrease incrementally the learning rate while increasing the number of trees.

subsample, which is for each tree the % of rows taken to build the tree. I recommend not taking out too many rows, as performance will drop a lot. Take values from 0.8 to 1.

colsample_bytree: number of columns used by each tree. In order to avoid some columns to take too much credit for the prediction (think of it like in recommender systems when you recommend the most purchased products and forget about the long tail), take out a good proportion of columns. Values from 0.3 to 0.8 if you have many columns (especially if you did one-hot encoding), or 0.8 to 1 if you only have a few columns.

gamma: usually misunderstood parameter, it acts as a regularization parameter. Either 0, 1 or 5.



